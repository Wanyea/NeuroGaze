Image detailing a top-down view of the International 10-20 system. There are nodes at each location with letter and number to describe location on the scalp.
On the left, there is an image of the EMOTIV Insight II EEG headset. On the right is the electrode positions of the EMOTIV Insight II (AF3, AF4, T7, T8, CMS, DRL, Pz) with respect to the International 10-20 system.
Image of polymer sensor used by the EMOTIV Insight II. On the left is a three pronged sensor, on the right is the one pronged sensor.
The image includes a picture of the Muse II EEG headband on the left with arrows pointing to the forehead sensors, reference sensors and SmartSense conductive ear sensors. On the right is the electrode location of the Muse II (TP9, TP10, FP1, FP2) with respect to the 10-20 International  System
The left side of this figure includes an visual of the Mind Monitor application connected to an EEG live stream from the Muse II. The right side of  this figure visualizes the Edge Impulse application machine learning model performance data with an accuracy of 40% and loss of 1.06. 
The left side of this figure shows an image of the EMOTIV EPOC X, the right side on the image shows the electrode locations (AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4, references: TP9, P3, P4, TP10) in respect to the 10-20 International System.
The image shows the EmotivPRO data stream playback user interface. There are multiple EEG brain wave charts in rows that are color coded for each of the 14 channels accessible by the EMOTIV EPOC X.
A screenshot of the EmotivBCI EEG Quality User Interface. This includes a top-down view of the electrode points on the users head that are each colored to represent the EEG score of each node. There is a percentage in the middle of the screen that represents the EEG quality. 
A screenshot of EmotivBCI Training Profile User Interface. Part A is on the left side of the image and shows the Brain Space Diagram with colored dots to represent the spread of the mental command, Part B is at the top of the image and shows a number represent the EEG quality and a battery logo showing how much life is left on the headset, Part C is towards the right side of the image and shows the user training profile name, Part D is also towards the right and shows mental commands to choose and train. 
This figure is a screenshot of the EmotivBCI Training User Face User Interface. It includes a bar and number towards the bottom of the image that shows the score from the training session. The screenshot also includes and Reject and Accept button that lets you choose what to do with the training data collected. 
A figure is the training loop diagram for the NeuroGaze system. The left side of the image shows the neutral command loop. Users must load the neutral VR Unity Scene, this is fed to the EmotivBCI program and data is sent to training profile. This is repeated 20 times. 
The right side of the image shows the pull command training loop. The Pull VR Unity Scene is loaded configured with the created training profile. The mental command button is clicked for the pull command and at the same time the interactable object starts shrinking. The training can be accepted or rejected. This is repeated for 20 successfully session.
The diagram is selection loop architecture diagram for the NeuroGaze. The loop starts with calibration of the eye trackers and the eye gaze position every frame and report it to Unity Game Engine. At the same time, a connection is made to EmotivBCI program and the headset configuration, training profile and subscribing to the data stream. The GetMentalCommand method is called each frame from the Unity Game Engine to pull the mental command from the EmotivBCI program. When eye gaze meets an interactable and the mental command is set to pull, the object is selected and shrinks until its destroyed.
The image shows the Unity NeuroGaze Evaluation Scene in Debug mode. The scene is organized with the Hierarchy tab on the left, the Inspector on the right and the Console and Project tab on the bottom.
The picture is a screenshot of the evaluation scene for VR controllers on the left, and the Eye Gaze combined with Hand Gesture on the right.
A screenshot of the Eye Tracking Ray C# script. Every frame, this script creates a new Vector3 for the center of the users eye, calculates the forward direction from the eyes and checks if a user's eye gaze meets an interactable object. If it does the interactable object will grow to set scale. If the eye gaze leaves the object, it will return to its original scale.
A screenshot of Eye Interactable C# script that is attached to every object in the scene we want users to be able to interact with. Every frame, this object checks if it is in hover mode (user eye gaze has met this object) and the mental command from the EmotivBCI program is pull. If both of these are true, the object is shrunk and destroyed, signifying selection. 
This screenshot shows the Mental Commands C# script which includes the configuration required to connect to the Emotiv EPOC X EEG headset. When user clicks the "1" key, the program establishes a connection to the headset defined in the "headsetId" variable, loads the training profile listed in the profileName variable, and the EEG data stream is connected to. 
A screenshot of the Mental Commands C# script that shows the mental command being read from the EEG data stream. If the mental command is the same as the last one received, do nothing. Else, report the mental command. 
A screen shot of the Interactable Manager C# script that shows how 12  interactable objects are chosen at random to turn red when the assessment begins. All the interactable objects are put into an array and 12 random indices are chosen.
A collage of a person wearing a virtual reality headset and an EEG headset. The first image is the person just wearing the EEG headset, the middle image is the person wearing both the EEG headset and the VR headset at the same time. The last image depicts the access point to one of the electrodes and saline solution being applied to this opening. 
A bar graph with error bars showing the Average Error by Input Device with the Input Device on the x-axis and the Error Count on the y-axis. From left to right: Eye Gaze + Hands Gesture is a blue bar reaching the 5.3. marker, NeuroGaze is an green bar reaching the 2.25 marker, VR controllers is a orange bar reaching the 4.15 marker.
A bar graph with error bars showing the NASA-TLX Scores by Input Device with the Input Device on the x-axis and Aggregated NASA-TLX Score on the y-axis. From left to right: Eye Gaze + Hands Gesture is a blue bar reaching the 20 marker,  VR controllers is a orange bar reaching the 19 marker, NeuroGaze is an green bar reaching the 16 marker.
A 2x3 grid of bar graph images. Each graph represents one of the individual subscale mean scales for each input device. Starting from the first row, moving left to right: the graph for input device vs. Mental Demand, input device vs Physical Demand, input device vs Temporal Demand, input device vs. Performance, input device vs Effort, input device vs. Frustration.
Institutional Review Board (IRB) approval document. Document lists all approved protocol forms, consent forms and supporting documents used in this Thesis.
Page 1 of the Demographics survey used in this study. Document lists participant number, age, gender identity, experience level with VR, AR and video games.
Page 2 of the Demographics survey used in this study. Document lists any virtual reality games the user has played, ingredients of saline solution used in experiment and a yes or no checkbox if the user is allergic to it, yes or no question for if user has previous experience with cyber sickness.
Page 1 of NASA-TLX survey used in this paper. A field for Participant Number is listed. A "Technique being evaluated" multiple choice question is listed with "NeuroGaze: EEG + Eye Tracking", " Eye Tracking + Hand Tracking", and "VR Controllers" being listed as options.
Page 2 of the NASA-TLX survey. A 7 point multiple choice question is listed where 1 is (very low) and 7 is (very high). The questions being asked on the 7 point scale are as follows:

1. How mentally demanding was the task? (Mental Demand)
2. How physically demanding was the task? (Physical Demand)
3. How hurried or rushed was the pace of the task? (Temporal Demand)
4. How successful were you in accomplishing what you were asked to do? (Performance)
5. How hard did you have to work to accomplish your level of performance? (Effort)
6. How insecure, discouraged, irritated, stressed and annoyed were you? (Frustration)

Page 1 of the Post Evaluation Survey. Participant number is listed. A yes or no question box is listed for the question: "Did you feel like you were making the cube shrink during the training scene?" A multiple choice question with a 1 (least intuitive) to 5 (most intutive) is included for the following questions: "How intuitive was NueroGaze to use?", "How intuitive was Eye Tracking combined with Hand Tracking to use?", "How intuitive was VR controllers to use?"
Page 2 of the Post Evaluation Survey. A blank field is under the question: "What would you change about any of the interaction techniques or the experiment? If nothing, answer Nothing." A 1-3 ranking question where 1 is best and 3 is worst is asked with the following question: "Which interaction technique did you prefer overall?" The options to rank are: NeuroGaze, Eye Tracking combined with Hand Tracking, VR Controllers.